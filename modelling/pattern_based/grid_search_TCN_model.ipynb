{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCV-0RKtE-Ea"
      },
      "outputs": [],
      "source": [
        "# when executed in a Google Colab setting, we must install the required libraries\n",
        "\n",
        "# !pip install torch\n",
        "# !pip install os\n",
        "# !pip install transformers\n",
        "# !pip install numpy\n",
        "# !pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfWnrSCtFDZZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils import weight_norm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pickle\n",
        "import base64\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEY-y5mGO0ny",
        "outputId": "e27306aa-32fe-40f6-f4ae-fcba2707b2f8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fb34f6e9f70>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#### Edit variables and filepaths here ####\n",
        "DATASET_FILEPATH = './drive/MyDrive/Thesis/'\n",
        "DATASET_SEED = 2\n",
        "SAVE_WEIGHTS_PATH = os.path.join(DATASET_FILEPATH, 'weights-and-graphs/grid-search-tcn/model.pth')\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EY2avNS7Bf-"
      },
      "outputs": [],
      "source": [
        "train_csv_file = os.path.join(DATASET_FILEPATH, f'base/{DATASET_SEED}/processed/train_dataset.csv')\n",
        "train_csv_file = os.path.join(DATASET_FILEPATH, f'base/{DATASET_SEED}/processed/validation_dataset.csv')\n",
        "aug_train_csv_file = os.path.join(DATASET_FILEPATH, '/base/aug-dataset/processed/train_dataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNgvRDXf7IRL"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print('Device: ', device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQlp2W9JFFZ-",
        "outputId": "05c03dc7-46ca-4a46-ca95-988591488847"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "def to_tensor(base64_str):\n",
        "    return pickle.loads(base64.b64decode(base64_str.encode()))\n",
        "\n",
        "\n",
        "selected_columns = ['audio_file_name','classification', 'wav2vec_embeddings', 'hubert_embeddings']\n",
        "train_df = pd.read_csv(train_csv_file, usecols=selected_columns, converters={'wav2vec_embeddings': to_tensor, 'hubert_embeddings' : to_tensor})\n",
        "validation_df = pd.read_csv(validation_csv_file, usecols=selected_columns, converters={'wav2vec_embeddings': to_tensor, 'hubert_embeddings' : to_tensor})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beb6AyuB7Rj-"
      },
      "outputs": [],
      "source": [
        "def process_training_set(train_df, oversample_minority=False, undersample_majority=False):\n",
        "  \"\"\"\n",
        "  Re-sample the training dataset, with options to oversample minority class and undersample majority class based on audio lengths.\n",
        "\n",
        "  :param train_df: DataFrame containing the training data with columns ['classification', 'audio_file_name'] among others.\n",
        "  :param oversample_minority: Boolean, if True, the minority class (classification == 0) is duplicated to balance the dataset.\n",
        "  :param undersample_majority: Boolean, if True, majority class data with audio lengths above a threshold (specified by DROP_SEGMENTS) are dropped.\n",
        "  :returns: DataFrame with the desired processed training data.\n",
        "  \"\"\"\n",
        "  if oversample_minority:\n",
        "    class_0 = train_df[train_df['classification'] == 0]\n",
        "    train_df = pd.concat([train_df, class_0])\n",
        "  if undersample_majority:\n",
        "    DROP_SEGMENTS = 5\n",
        "    def get_audio_length_group(file_name):\n",
        "        return int(re.findall(r'\\d+', file_name)[-1])\n",
        "    train_df['audio_length_group'] = train_df['audio_file_name'].apply(get_audio_length_group)\n",
        "    train_df = train_df[train_df['audio_length_group'] <= DROP_SEGMENTS]\n",
        "    train_df = train_df.drop(columns=['audio_length_group'])\n",
        "\n",
        "  # some indices are duplicated / removed so we have to reset them\n",
        "  train_df.reset_index(drop=True, inplace=True)\n",
        "  return train_df\n",
        "\n",
        "def print_dataset_balance(df):\n",
        "    \"\"\"\n",
        "    Prints the balance of classifications in a given dataset.\n",
        "\n",
        "    :param df: DataFrame containing the data with a 'classification' column.\n",
        "    \"\"\"\n",
        "    classification_counts = df['classification'].value_counts().reset_index()\n",
        "    classification_counts.columns = ['classification', 'count']\n",
        "    total_rows = classification_counts['count'].sum()\n",
        "    classification_counts['percentage'] = (classification_counts['count'] / total_rows) * 100\n",
        "    classification_counts['percentage'] = classification_counts['percentage'].round(1)\n",
        "    print(classification_counts)\n",
        "\n",
        "def augment_train_dataset(df, augmented_df_filepath):\n",
        "  \"\"\"\n",
        "  Introduce additional 'non-interruption' samples to the dataset, which have been extracted from the GAP dataset with an LLM.\n",
        "\n",
        "  :param df: Original DataFrame containing the training data.\n",
        "  :param augmented_df_filepath: Filepath to the CSV containing the augmented data.\n",
        "  :returns: A combined DataFrame of the original and augmented training data.\n",
        "  \"\"\"\n",
        "  selected_columns = ['audio_file_name','classification', 'wav2vec_embeddings', 'hubert_embeddings']\n",
        "  aug_train_df = pd.read_csv(aug_train_csv_file, usecols=selected_columns, converters={'wav2vec_embeddings': to_tensor, 'hubert_embeddings' : to_tensor})\n",
        "  augmented_df = pd.concat([df, aug_train_df], ignore_index=True)\n",
        "  return augmented_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0psKaz5uFOH1"
      },
      "outputs": [],
      "source": [
        "# Creating the Dataset\n",
        "class AudioEmbeddingsDataset(Dataset):\n",
        "    def __init__(self, embeddings, labels):\n",
        "        self.embeddings = embeddings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.embeddings)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        embedding = self.embeddings[idx]\n",
        "        label = self.labels[idx]\n",
        "        return label, embedding\n",
        "\n",
        "# Creating DataLoader with custom collate function\n",
        "FIXED_LENGTH = 250 # fixed sequence length that the model expects as an input\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Function to be passed to the DataLoader class which processes a batch of data points before being passed to the model in training.\n",
        "    The TCN must process process data points of length 250, we adjust each data point in the batch to fit this requirement.\n",
        "\n",
        "    :param batch: array of data points in the dataset.\n",
        "    \"\"\"\n",
        "    labels, embeddings = zip(*batch)\n",
        "    labels = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "    # Truncate or zero-pad all sequences to a fixed length\n",
        "    embeddings = [emb.squeeze(0) for emb in embeddings]\n",
        "    embeddings = [emb[:FIXED_LENGTH, :] if emb.shape[0] > FIXED_LENGTH else torch.cat([emb, torch.zeros((FIXED_LENGTH - emb.shape[0], emb.shape[1]))]) for emb in embeddings]\n",
        "\n",
        "    embeddings = torch.stack(embeddings)\n",
        "    return embeddings, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsXgKm08Vyaw"
      },
      "outputs": [],
      "source": [
        "class NormReLUChannelNormalization(nn.Module):\n",
        "    def __init__(self, epsilon=1e-5):\n",
        "        super(NormReLUChannelNormalization, self).__init__()\n",
        "        self.epsilon = epsilon\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(x)\n",
        "        max_values, _ = torch.max(torch.abs(x), dim=2, keepdim=True)\n",
        "        max_values += self.epsilon\n",
        "        out = x / max_values\n",
        "        return out\n",
        "\n",
        "class WaveNetActivation(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(WaveNetActivation, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        tanh_out = torch.tanh(x)\n",
        "        sigm_out = torch.sigmoid(x)\n",
        "        return tanh_out * sigm_out\n",
        "\n",
        "class Chomp1d(nn.Module):\n",
        "    def __init__(self, chomp_size):\n",
        "        super(Chomp1d, self).__init__()\n",
        "        self.chomp_size = chomp_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x[:, :, :-self.chomp_size].contiguous()\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, dilation, kernel_size, activation, dropout=0):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        chomp_size = (kernel_size-1) * dilation\n",
        "        padding = (kernel_size-1) * dilation\n",
        "        self.conv1 = weight_norm(nn.Conv1d(in_channels, out_channels, kernel_size,\n",
        "                                           stride=1, padding=padding, dilation=dilation))\n",
        "        self.chomp1 = Chomp1d(chomp_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.activation = activation\n",
        "        self.conv2 = weight_norm(nn.Conv1d(out_channels, out_channels, kernel_size,\n",
        "                                           stride=1, padding=padding, dilation=dilation))\n",
        "        self.chomp2 = Chomp1d(chomp_size)\n",
        "        self.net = nn.Sequential(self.conv1, self.chomp1, self.activation, self.dropout,\n",
        "                                 self.conv2, self.chomp2, self.activation, self.dropout)\n",
        "        self.downsample = nn.Conv1d(in_channels, out_channels, 1) if in_channels != out_channels else None\n",
        "        self.relu = nn.ReLU()\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        self.conv1.weight.data.normal_(0, 0.01)\n",
        "        self.conv2.weight.data.normal_(0, 0.01)\n",
        "        if self.downsample is not None:\n",
        "            self.downsample.weight.data.normal_(0, 0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.net(x)\n",
        "        res = x if self.downsample is None else self.downsample(x)\n",
        "        return self.relu(out + res)\n",
        "\n",
        "class TemporalConvNet(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=2, dropout=0.2):\n",
        "        super(TemporalConvNet, self).__init__()\n",
        "        layers = []\n",
        "        num_levels = len(out_channels)\n",
        "        for i in range(num_levels):\n",
        "            dilation_size = 2 ** i\n",
        "            in_channels = in_channels if i == 0 else out_channels[i-1]\n",
        "            activation = NormReLUChannelNormalization() if i%2 == 0 else WaveNetActivation()\n",
        "            layers += [ResidualBlock(in_channels, out_channels[i], dilation=dilation_size,\n",
        "                                     kernel_size=kernel_size, activation=activation, dropout=dropout)]\n",
        "\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "class TCN(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=2, dropout=0.3):\n",
        "        super(TCN, self).__init__()\n",
        "        self.tcn = TemporalConvNet(in_channels, out_channels, kernel_size=kernel_size, dropout=dropout)\n",
        "        self.linear = nn.Linear(out_channels[-1], 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)\n",
        "        y1 = self.tcn(x)\n",
        "        o = self.linear(y1[:, :, -1])\n",
        "        return o\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEEREQVMGes3"
      },
      "outputs": [],
      "source": [
        "# Model training code\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Receives predicted values and true labels and computes the average accuracy of the predictions.\n",
        "    Should 8/10 be correctly classified, this returns 0.8, NOT 8\n",
        "\n",
        "    :param preds: Tensor of predicted values.\n",
        "    :param y: Tensor of true labels.\n",
        "    :returns: Accuracy as a floating point value.\n",
        "    \"\"\"\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds)).squeeze()\n",
        "    correct = (rounded_preds == y).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \"\"\"\n",
        "    Evaluate the model's performance on a given dataset. This is used for the validation\n",
        "\n",
        "    :param model: PyTorch model to be evaluated.\n",
        "    :param iterator: Iterator that provides batches of data for evaluation.\n",
        "    :param criterion: Loss function used to compute the loss during evaluation.\n",
        "    :returns: Tuple containing average loss, average accuracy and macro average F1 score over all batches.\n",
        "    \"\"\"\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for embeddings, labels in iterator:\n",
        "            embeddings, labels = embeddings.to(device), labels.to(device)\n",
        "            predictions = model(embeddings).squeeze(1)\n",
        "            loss = criterion(predictions, labels)\n",
        "            acc = binary_accuracy(predictions, labels)\n",
        "\n",
        "            binary_predictions = (torch.sigmoid(predictions) > 0.5).int()\n",
        "\n",
        "            all_predictions.extend(binary_predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        macro_f1 = f1_score(all_labels, all_predictions, average='macro')\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator), macro_f1\n",
        "\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    \"\"\"\n",
        "    Train the model for one epoch on the dataset.\n",
        "\n",
        "    :param model: The PyTorch model to be trained.\n",
        "    :param iterator: Iterator that provides batches of data for training.\n",
        "    :param optimizer: Optimizer used to update the model's parameters.\n",
        "    :param criterion: Loss function used to compute the loss during training.\n",
        "    :returns: Tuple containing average loss, average accuracy over all batches, and total number of batches.\n",
        "    \"\"\"\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, (embeddings, labels) in enumerate(iterator):\n",
        "        embeddings, labels = embeddings.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(embeddings).squeeze(1)\n",
        "        loss = criterion(predictions, labels)\n",
        "        acc = binary_accuracy(predictions, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "        # Print or log the loss and accuracy for every batch\n",
        "        # print(f\"Batch {batch_idx + 1}/{len(iterator)} - Loss: {loss.item():.4f}, Accuracy: {acc.item():.4f}\")\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HR8ImpXuq18b"
      },
      "outputs": [],
      "source": [
        "param_grid = {\n",
        "    'learning-rate': [0.001],\n",
        "    'batch-size': [16],\n",
        "    'optimiser' : ['Adam', 'SGD'],\n",
        "    'tcn' : [\n",
        "        {\n",
        "            # small\n",
        "            'layers' : [512, 256],\n",
        "            'dropout-rate' : 0.2,\n",
        "        },\n",
        "        {\n",
        "            # medium\n",
        "            'layers' : [768, 384, 192],\n",
        "            'dropout-rate' : 0.3,\n",
        "        },\n",
        "        {\n",
        "            # large\n",
        "            'layers' : [1024, 768, 384],\n",
        "            'dropout-rate' : 0.4,\n",
        "        }\n",
        "    ],\n",
        "    'kernel-size' : [2, 3],\n",
        "    'embeddings' : ['wav2vec', 'hubert'],\n",
        "    'dataset' : ['method 1', 'method 2']\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jQ-IfK8GjH4"
      },
      "outputs": [],
      "source": [
        "def train_model(gridsearch_params):\n",
        "  \"\"\"\n",
        "  Trains a classifier model on audio embeddings (either wav2vec or hubert) based on parameters received\n",
        "  from a grid search. One of the key hyperparameters is the method which dictates the strategy to balance the dataset.\n",
        "\n",
        "  Depending on the chosen dataset method, this function either:\n",
        "  1. Over-samples the minority class, under-samples the majority class, and optionally prunes the dataset (referred to as \"method 1\"), or\n",
        "  2. Augments the dataset with instances of False interruptions (referred to as \"method 2\").\n",
        "\n",
        "  Checkpoints are taken based on the macro average F1 score. The function returns the model weights of the\n",
        "  epoch which has the best score beyond the fifth epoch. The early stopping condition is met when there are\n",
        "  three consecutive falls in the macro average F1 score after the fifth epoch.\n",
        "\n",
        "  :param gridsearch_params: A dictionary containing parameters sourced from a grid search. Key parameters\n",
        "                            include 'dataset' (which determines the chosen method of dataset processing),\n",
        "                            'embeddings', 'batch-size', 'tcn' (which further includes 'layers' and 'dropout-rate'),\n",
        "                            'kernel-size', 'optimiser', and 'learning-rate'.\n",
        "  :returns: A tuple containing (1) the best model weights during training, corresponding to the epoch with\n",
        "            the highest macro average F1 score beyond the fifth epoch, and (2) that highest Macro-weighted\n",
        "            average F1 score.\n",
        "  \"\"\"\n",
        "  if gridsearch_params['dataset'] == 'method 2':\n",
        "    balanced_train_df = augment_train_dataset(train_df.copy(deep=True), aug_train_csv_file)\n",
        "  else:\n",
        "    balanced_train_df = process_training_set(train_df.copy(deep=True), oversample_minority=True, undersample_majority=True, prune=False)\n",
        "\n",
        "  copy_validation_df = validation_df.copy(deep=True)\n",
        "\n",
        "  wav2vec_train_data, wav2vec_valid_data = balanced_train_df['wav2vec_embeddings'], copy_validation_df['wav2vec_embeddings']\n",
        "  hubert_train_data, hubert_valid_data = balanced_train_df['hubert_embeddings'], copy_validation_df['hubert_embeddings']\n",
        "  train_labels, valid_labels = balanced_train_df['classification'], copy_validation_df['classification']\n",
        "\n",
        "  if gridsearch_params['embeddings'] == 'wav2vec':\n",
        "    train_dataset = AudioEmbeddingsDataset(wav2vec_train_data, train_labels)\n",
        "    valid_dataset = AudioEmbeddingsDataset(wav2vec_valid_data, valid_labels)\n",
        "  else:\n",
        "    train_dataset = AudioEmbeddingsDataset(hubert_train_data, train_labels)\n",
        "    valid_dataset = AudioEmbeddingsDataset(hubert_valid_data, valid_labels)\n",
        "\n",
        "  BATCH_SIZE = gridsearch_params['batch-size']\n",
        "  train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "  valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "  classifier_model = TCN(768, gridsearch_params['tcn']['layers'], kernel_size=gridsearch_params['kernel-size'], dropout=gridsearch_params['tcn']['dropout-rate']).to(device)\n",
        "  if gridsearch_params['optimiser'] == 'Adam':\n",
        "    optimizer = torch.optim.Adam(classifier_model.parameters(), lr=gridsearch_params['learning-rate'])\n",
        "  else:\n",
        "    optimizer = torch.optim.SGD(classifier_model.parameters(), lr=gridsearch_params['learning-rate'], momentum=0.9)\n",
        "\n",
        "  train_losses = []\n",
        "  valid_losses = []\n",
        "  macro_f1_scores = []\n",
        "  best_model_weights = None\n",
        "  best_macro_f1 = -1\n",
        "\n",
        "  MAX_EPOCHS = 20\n",
        "  consecutive_rises = 0  # keep track of consecutive drops in macro avg F1 score\n",
        "\n",
        "  for epoch in range(MAX_EPOCHS):\n",
        "      train_loss, train_acc = train(classifier_model, train_loader, optimizer, criterion)\n",
        "      valid_loss, valid_acc, epoch_macro_f1 = evaluate(classifier_model, valid_loader, criterion)\n",
        "\n",
        "      train_losses.append(train_loss)\n",
        "      valid_losses.append(valid_loss)\n",
        "      macro_f1_scores.append(epoch_macro_f1)\n",
        "\n",
        "      if epoch_macro_f1 > best_macro_f1 and epoch >= 5:\n",
        "        best_macro_f1 = epoch_macro_f1\n",
        "        best_model_weights = classifier_model.state_dict()\n",
        "\n",
        "      if epoch > 0 and epoch_macro_f1 < macro_f1_scores[-2]: # early stop if 3 consecutive rises in validation loss\n",
        "          consecutive_rises += 1\n",
        "      else:\n",
        "          consecutive_rises = 0\n",
        "\n",
        "      if consecutive_rises >= 3 and epoch >= 4: # ensure at least 5 epochs are completed\n",
        "          break\n",
        "\n",
        "  validation_performance = max(macro_f1_scores[4:]) # only count losses from the 5th epoch onwards\n",
        "\n",
        "  del balanced_train_df, copy_validation_df\n",
        "  print('Finished training model: ', gridsearch_params, ', with highest Macro-weighted average F1 score: ', validation_performance)\n",
        "  return best_model_weights, validation_performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKqCdEBistma",
        "outputId": "a4d36ce5-b2bc-4ce7-ab0c-57add6515638"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'Adam', 'tcn': {'layers': [512, 256], 'dropout-rate': 0.2}, 'kernel-size': 2, 'embeddings': 'wav2vec', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.5940071468380159\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'Adam', 'tcn': {'layers': [512, 256], 'dropout-rate': 0.2}, 'kernel-size': 2, 'embeddings': 'wav2vec', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.6204624415957829\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'Adam', 'tcn': {'layers': [512, 256], 'dropout-rate': 0.2}, 'kernel-size': 2, 'embeddings': 'hubert', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.7096774193548386\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'Adam', 'tcn': {'layers': [512, 256], 'dropout-rate': 0.2}, 'kernel-size': 2, 'embeddings': 'hubert', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.6515679442508711\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'Adam', 'tcn': {'layers': [512, 256], 'dropout-rate': 0.2}, 'kernel-size': 3, 'embeddings': 'wav2vec', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.6064147259796402\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'Adam', 'tcn': {'layers': [512, 256], 'dropout-rate': 0.2}, 'kernel-size': 3, 'embeddings': 'wav2vec', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.5953432356483339\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'Adam', 'tcn': {'layers': [512, 256], 'dropout-rate': 0.2}, 'kernel-size': 3, 'embeddings': 'hubert', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.5391149135224965\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'Adam', 'tcn': {'layers': [512, 256], 'dropout-rate': 0.2}, 'kernel-size': 3, 'embeddings': 'hubert', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.6806451612903226\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'Adam', 'tcn': {'layers': [768, 384, 192], 'dropout-rate': 0.3}, 'kernel-size': 2, 'embeddings': 'wav2vec', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.5499999999999999\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'Adam', 'tcn': {'layers': [768, 384, 192], 'dropout-rate': 0.3}, 'kernel-size': 2, 'embeddings': 'wav2vec', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.6651162790697674\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'Adam', 'tcn': {'layers': [768, 384, 192], 'dropout-rate': 0.3}, 'kernel-size': 2, 'embeddings': 'hubert', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.6368647919537871\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'Adam', 'tcn': {'layers': [768, 384, 192], 'dropout-rate': 0.3}, 'kernel-size': 2, 'embeddings': 'hubert', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.674078018905605\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'Adam', 'tcn': {'layers': [768, 384, 192], 'dropout-rate': 0.3}, 'kernel-size': 3, 'embeddings': 'wav2vec', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.5089646260701017\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'Adam', 'tcn': {'layers': [768, 384, 192], 'dropout-rate': 0.3}, 'kernel-size': 3, 'embeddings': 'wav2vec', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.6259073427909227\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'Adam', 'tcn': {'layers': [768, 384, 192], 'dropout-rate': 0.3}, 'kernel-size': 3, 'embeddings': 'hubert', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.6342638073559362\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'Adam', 'tcn': {'layers': [768, 384, 192], 'dropout-rate': 0.3}, 'kernel-size': 3, 'embeddings': 'hubert', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.6910220214568041\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'Adam', 'tcn': {'layers': [1024, 768, 384], 'dropout-rate': 0.4}, 'kernel-size': 2, 'embeddings': 'wav2vec', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.5399454049135578\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'Adam', 'tcn': {'layers': [1024, 768, 384], 'dropout-rate': 0.4}, 'kernel-size': 2, 'embeddings': 'wav2vec', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.6217190943698657\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'Adam', 'tcn': {'layers': [1024, 768, 384], 'dropout-rate': 0.4}, 'kernel-size': 2, 'embeddings': 'hubert', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.7411694218252132\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'Adam', 'tcn': {'layers': [1024, 768, 384], 'dropout-rate': 0.4}, 'kernel-size': 2, 'embeddings': 'hubert', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.599200685127034\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'Adam', 'tcn': {'layers': [1024, 768, 384], 'dropout-rate': 0.4}, 'kernel-size': 3, 'embeddings': 'wav2vec', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.5016496320051681\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'Adam', 'tcn': {'layers': [1024, 768, 384], 'dropout-rate': 0.4}, 'kernel-size': 3, 'embeddings': 'wav2vec', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.6397604790419161\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'Adam', 'tcn': {'layers': [1024, 768, 384], 'dropout-rate': 0.4}, 'kernel-size': 3, 'embeddings': 'hubert', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.6992880873279544\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'Adam', 'tcn': {'layers': [1024, 768, 384], 'dropout-rate': 0.4}, 'kernel-size': 3, 'embeddings': 'hubert', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.7154150197628457\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'SGD', 'tcn': {'layers': [512, 256], 'dropout-rate': 0.2}, 'kernel-size': 2, 'embeddings': 'wav2vec', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.5375392520696547\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'SGD', 'tcn': {'layers': [512, 256], 'dropout-rate': 0.2}, 'kernel-size': 2, 'embeddings': 'wav2vec', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.6433724561214601\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'SGD', 'tcn': {'layers': [512, 256], 'dropout-rate': 0.2}, 'kernel-size': 2, 'embeddings': 'hubert', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.62615518744551\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'SGD', 'tcn': {'layers': [512, 256], 'dropout-rate': 0.2}, 'kernel-size': 2, 'embeddings': 'hubert', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.6714805328971438\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'SGD', 'tcn': {'layers': [512, 256], 'dropout-rate': 0.2}, 'kernel-size': 3, 'embeddings': 'wav2vec', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.572219991971096\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'SGD', 'tcn': {'layers': [512, 256], 'dropout-rate': 0.2}, 'kernel-size': 3, 'embeddings': 'wav2vec', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.59899389148401\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'SGD', 'tcn': {'layers': [512, 256], 'dropout-rate': 0.2}, 'kernel-size': 3, 'embeddings': 'hubert', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.619974782277219\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'SGD', 'tcn': {'layers': [512, 256], 'dropout-rate': 0.2}, 'kernel-size': 3, 'embeddings': 'hubert', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.6714805328971438\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'SGD', 'tcn': {'layers': [768, 384, 192], 'dropout-rate': 0.3}, 'kernel-size': 2, 'embeddings': 'wav2vec', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.5677584532532886\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'SGD', 'tcn': {'layers': [768, 384, 192], 'dropout-rate': 0.3}, 'kernel-size': 2, 'embeddings': 'wav2vec', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.5876370099733387\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'SGD', 'tcn': {'layers': [768, 384, 192], 'dropout-rate': 0.3}, 'kernel-size': 2, 'embeddings': 'hubert', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.6589434661723819\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'SGD', 'tcn': {'layers': [768, 384, 192], 'dropout-rate': 0.3}, 'kernel-size': 2, 'embeddings': 'hubert', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.6557083084279737\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'SGD', 'tcn': {'layers': [768, 384, 192], 'dropout-rate': 0.3}, 'kernel-size': 3, 'embeddings': 'wav2vec', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.49652913321678915\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'SGD', 'tcn': {'layers': [768, 384, 192], 'dropout-rate': 0.3}, 'kernel-size': 3, 'embeddings': 'wav2vec', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.627156405238597\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'SGD', 'tcn': {'layers': [768, 384, 192], 'dropout-rate': 0.3}, 'kernel-size': 3, 'embeddings': 'hubert', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.6695407009146641\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'SGD', 'tcn': {'layers': [768, 384, 192], 'dropout-rate': 0.3}, 'kernel-size': 3, 'embeddings': 'hubert', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.680253766851705\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'SGD', 'tcn': {'layers': [1024, 768, 384], 'dropout-rate': 0.4}, 'kernel-size': 2, 'embeddings': 'wav2vec', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.5368788142981692\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'SGD', 'tcn': {'layers': [1024, 768, 384], 'dropout-rate': 0.4}, 'kernel-size': 2, 'embeddings': 'wav2vec', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.6526181353767561\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'SGD', 'tcn': {'layers': [1024, 768, 384], 'dropout-rate': 0.4}, 'kernel-size': 2, 'embeddings': 'hubert', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.669462346605456\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'SGD', 'tcn': {'layers': [1024, 768, 384], 'dropout-rate': 0.4}, 'kernel-size': 2, 'embeddings': 'hubert', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.6415897230028101\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'SGD', 'tcn': {'layers': [1024, 768, 384], 'dropout-rate': 0.4}, 'kernel-size': 3, 'embeddings': 'wav2vec', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.5684640489336809\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'SGD', 'tcn': {'layers': [1024, 768, 384], 'dropout-rate': 0.4}, 'kernel-size': 3, 'embeddings': 'wav2vec', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.6149994335561346\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'SGD', 'tcn': {'layers': [1024, 768, 384], 'dropout-rate': 0.4}, 'kernel-size': 3, 'embeddings': 'hubert', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.6749999999999999\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'SGD', 'tcn': {'layers': [1024, 768, 384], 'dropout-rate': 0.4}, 'kernel-size': 3, 'embeddings': 'hubert', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.7285994447698851\n",
            "\n",
            "Optimal hyperparameters for grid search with macro average F1 of  0.7411694218252132  :\n",
            "{'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'Adam', 'tcn': {'layers': [1024, 768, 384], 'dropout-rate': 0.4}, 'kernel-size': 2, 'embeddings': 'hubert', 'dataset': 'method 1'}\n"
          ]
        }
      ],
      "source": [
        "from itertools import product\n",
        "\n",
        "def grid_search(param_grid):\n",
        "    \"\"\"\n",
        "    Conducts a grid search over the specified parameter space to find the best parameters that maximize\n",
        "    the performance of the `train_model` function.\n",
        "\n",
        "    For every combination of parameters in the grid, the function trains the model and saves the best\n",
        "    model weights (based on macro average F1 score beyond the fifth epoch) if the current combination\n",
        "    yields better performance than previous ones. This approach ensures that the optimal hyperparameters\n",
        "    and corresponding model weights are identified and saved.\n",
        "\n",
        "    :param param_grid: A dictionary where keys are parameter names and values are lists of possible values\n",
        "                       for that parameter.\n",
        "    :returns: A tuple containing (1) a dictionary of the best parameters identified during the grid search\n",
        "              and (2) the highest macro average F1 score obtained using those best parameters.\n",
        "    \"\"\"\n",
        "    # create a list of all parameter combinations\n",
        "    all_params = [dict(zip(param_grid.keys(), v)) for v in product(*param_grid.values())]\n",
        "    best_params = None\n",
        "    best_performance = float('-inf')  # since we are maximising F1 score\n",
        "\n",
        "    for params in all_params:\n",
        "        best_weights, performance = train_model(params)\n",
        "        if performance > best_performance:\n",
        "            best_performance = performance\n",
        "            best_params = params\n",
        "            torch.save(best_weights, SAVE_WEIGHTS_PATH)\n",
        "\n",
        "    return best_params, best_performance\n",
        "\n",
        "best_hyperparameters, best_performance = grid_search(param_grid)\n",
        "print('\\nOptimal hyperparameters for grid search with macro average F1 of ',  best_performance,' :')\n",
        "print(best_hyperparameters)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
