{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aPWXEi-FTGl"
      },
      "outputs": [],
      "source": [
        "# when executed in a Google Colab setting, we must install the required libraries\n",
        "\n",
        "# !pip install torch\n",
        "# !pip install os\n",
        "# !pip install transformers\n",
        "# !pip install numpy\n",
        "# !pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCoFa99OFjv1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.distributions.beta import Beta\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import pickle\n",
        "import base64\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmiSqAaYHX47"
      },
      "outputs": [],
      "source": [
        "DATASET_FILEPATH = './drive/MyDrive/Thesis/'\n",
        "DATASET_SEED = 2\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "EMB_SIZE = 'base' # 'base' 768 embeddings or 'large' 1024 embeddings\n",
        "SAVE_WEIGHTS_PATH = os.path.join(DATASET_FILEPATH, 'weights-and-graphs/grid-search-avg/model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDeIrJMqAPXi"
      },
      "outputs": [],
      "source": [
        "train_csv_file = os.path.join(DATASET_FILEPATH, f'{EMB_SIZE}/{DATASET_SEED}/processed/train_dataset.csv')\n",
        "validation_csv_file = os.path.join(DATASET_FILEPATH, f'{EMB_SIZE}/{DATASET_SEED}/processed/validation_dataset.csv')\n",
        "aug_train_csv_file = os.path.join(DATASET_FILEPATH, '/base/aug-dataset/processed/train_dataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gv9Y8tRFba9",
        "outputId": "6b79626f-f4f3-4c6d-f968-76945490f04d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print('Device: ', device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlrdyKpSFtim"
      },
      "outputs": [],
      "source": [
        "def to_tensor(base64_str):\n",
        "    return pickle.loads(base64.b64decode(base64_str.encode()))\n",
        "\n",
        "selected_columns = ['audio_file_name', 'classification', 'wav2vec_embeddings', 'hubert_embeddings', 'bert_embeddings']\n",
        "\n",
        "train_df = pd.read_csv(train_csv_file, usecols=selected_columns, converters={'hubert_embeddings': to_tensor, 'wav2vec_embeddings' : to_tensor})\n",
        "validation_df = pd.read_csv(validation_csv_file, usecols=selected_columns, converters={'hubert_embeddings': to_tensor, 'wav2vec_embeddings' : to_tensor})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbsKgHA9F6r3"
      },
      "outputs": [],
      "source": [
        "def process_training_set(train_df, oversample_minority=False, undersample_majority=False):\n",
        "  \"\"\"\n",
        "  Re-sample the training dataset, with options to oversample minority class and undersample majority class based on audio lengths.\n",
        "\n",
        "  :param train_df: DataFrame containing the training data with columns ['classification', 'audio_file_name'] among others.\n",
        "  :param oversample_minority: Boolean, if True, the minority class (classification == 0) is duplicated to balance the dataset.\n",
        "  :param undersample_majority: Boolean, if True, majority class data with audio lengths above a threshold (specified by DROP_SEGMENTS) are dropped.\n",
        "  :returns: DataFrame with the desired processed training data.\n",
        "  \"\"\"\n",
        "  if oversample_minority:\n",
        "    class_0 = train_df[train_df['classification'] == 0]\n",
        "    train_df = pd.concat([train_df, class_0])\n",
        "  if undersample_majority:\n",
        "    DROP_SEGMENTS = 5\n",
        "    def get_audio_length_group(file_name):\n",
        "        return int(re.findall(r'\\d+', file_name)[-1])\n",
        "    train_df['audio_length_group'] = train_df['audio_file_name'].apply(get_audio_length_group)\n",
        "    train_df = train_df[train_df['audio_length_group'] <= DROP_SEGMENTS]\n",
        "    train_df = train_df.drop(columns=['audio_length_group'])\n",
        "\n",
        "  # some indices are duplicated / removed so we have to reset them\n",
        "  train_df.reset_index(drop=True, inplace=True)\n",
        "  return train_df\n",
        "\n",
        "def print_dataset_balance(df):\n",
        "    \"\"\"\n",
        "    Prints the balance of classifications in a given dataset.\n",
        "\n",
        "    :param df: DataFrame containing the data with a 'classification' column.\n",
        "    \"\"\"\n",
        "    classification_counts = df['classification'].value_counts().reset_index()\n",
        "    classification_counts.columns = ['classification', 'count']\n",
        "    total_rows = classification_counts['count'].sum()\n",
        "    classification_counts['percentage'] = (classification_counts['count'] / total_rows) * 100\n",
        "    classification_counts['percentage'] = classification_counts['percentage'].round(1)\n",
        "    print(classification_counts)\n",
        "\n",
        "def augment_train_dataset(df, augmented_df_filepath):\n",
        "  \"\"\"\n",
        "  Introduce additional 'non-interruption' samples to the dataset, which have been extracted from the GAP dataset with an LLM.\n",
        "\n",
        "  :param df: Original DataFrame containing the training data.\n",
        "  :param augmented_df_filepath: Filepath to the CSV containing the augmented data.\n",
        "  :returns: A combined DataFrame of the original and augmented training data.\n",
        "  \"\"\"\n",
        "  selected_columns = ['audio_file_name','classification', 'wav2vec_embeddings', 'hubert_embeddings']\n",
        "  aug_train_df = pd.read_csv(aug_train_csv_file, usecols=selected_columns, converters={'wav2vec_embeddings': to_tensor, 'hubert_embeddings' : to_tensor})\n",
        "  augmented_df = pd.concat([df, aug_train_df], ignore_index=True)\n",
        "  return augmented_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ViRRjTHgHaNn"
      },
      "outputs": [],
      "source": [
        "class EmbeddingsDataset(Dataset):\n",
        "    def __init__(self, audio_embeddings, labels):\n",
        "        self.audio_embeddings = audio_embeddings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_embeddings)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_embedding = self.audio_embeddings[idx]\n",
        "        label = self.labels[idx]\n",
        "        return audio_embedding, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuULbfxtH1sh"
      },
      "outputs": [],
      "source": [
        "class AudioModel(nn.Module):\n",
        "    def __init__(self, audio_embedding_dim=768, hidden_dims=[256], output_dim=1, dropout_rate=0):\n",
        "        super(AudioModel, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        prev_dim = audio_embedding_dim\n",
        "        for dim in hidden_dims:\n",
        "            layers.extend([\n",
        "                nn.Linear(prev_dim, dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout_rate)\n",
        "            ])\n",
        "            prev_dim = dim\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n",
        "\n",
        "    def forward(self, audio_embedding):\n",
        "        out = self.model(audio_embedding)\n",
        "        return self.output_layer(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AEPuVPmH6mz"
      },
      "outputs": [],
      "source": [
        "criterion = nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qg4NQLTvH85U"
      },
      "outputs": [],
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Receives predicted values and true labels and computes the average accuracy of the predictions.\n",
        "    Should 8/10 be correctly classified, this returns 0.8, NOT 8\n",
        "\n",
        "    :param preds: Tensor of predicted values.\n",
        "    :param y: Tensor of true labels.\n",
        "    :returns: Accuracy as a floating point value.\n",
        "    \"\"\"\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float()  # convert into float for division\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \"\"\"\n",
        "    Evaluate the model's performance on a given dataset. This is used for the validation\n",
        "\n",
        "    :param model: PyTorch model to be evaluated.\n",
        "    :param iterator: Iterator that provides batches of data for evaluation.\n",
        "    :param criterion: Loss function used to compute the loss during evaluation.\n",
        "    :returns: Tuple containing average loss, average accuracy and macro average F1 score over all batches.\n",
        "    \"\"\"\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.eval()\n",
        "\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for audio_embeddings, labels in iterator:\n",
        "            predictions = model(audio_embeddings).squeeze(1)\n",
        "            loss = criterion(predictions, labels.float())\n",
        "            acc = binary_accuracy(predictions, labels)\n",
        "\n",
        "            binary_predictions = (torch.sigmoid(predictions) > 0.5).int()\n",
        "\n",
        "            all_predictions.extend(binary_predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        macro_f1 = f1_score(all_labels, all_predictions, average='macro')\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator), macro_f1\n",
        "\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    \"\"\"\n",
        "    Train the model for one epoch on the dataset.\n",
        "\n",
        "    :param model: The PyTorch model to be trained.\n",
        "    :param iterator: Iterator that provides batches of data for training.\n",
        "    :param optimizer: Optimizer used to update the model's parameters.\n",
        "    :param criterion: Loss function used to compute the loss during training.\n",
        "    :returns: Tuple containing average loss, average accuracy over all batches, and total number of batches.\n",
        "    \"\"\"\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.train()\n",
        "\n",
        "    for audio_embeddings, labels in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(audio_embeddings).squeeze(1)\n",
        "        loss = criterion(predictions, labels.float())\n",
        "        acc = binary_accuracy(predictions, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRjCROJW6zhC"
      },
      "outputs": [],
      "source": [
        "param_grid = {\n",
        "    'learning-rate': [0.0005, 0.001],\n",
        "    'batch-size': [16],\n",
        "    'optimiser' : ['Adam', 'SGD'],\n",
        "    'architecture' : [\n",
        "        {\n",
        "            'layers' : [512,256],\n",
        "            'dropout-rate': 0.2,\n",
        "        },\n",
        "        {\n",
        "            'layers' : [768, 512, 256],\n",
        "            'dropout-rate': 0.3,\n",
        "        },\n",
        "        {\n",
        "            'layers' : [1024, 768, 512, 256],\n",
        "            'dropout-rate': 0.4,\n",
        "        },\n",
        "    ],\n",
        "    'embeddings' : ['wav2vec', 'hubert'],\n",
        "    'dataset' : ['method 1', 'method 2']\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFt9bm7nIAVn"
      },
      "outputs": [],
      "source": [
        "def train_model(gridsearch_params):\n",
        "  \"\"\"\n",
        "  Trains a classifier model on audio embeddings (either wav2vec or hubert) based on parameters received\n",
        "  from a grid search. One of the key hyperparameters is the method which dictates the strategy to balance the dataset.\n",
        "\n",
        "  Depending on the chosen dataset method, this function either:\n",
        "  1. Over-samples the minority class, under-samples the majority class, and optionally prunes the dataset (referred to as \"method 1\"), or\n",
        "  2. Augments the dataset with instances of False interruptions (referred to as \"method 2\").\n",
        "\n",
        "  Checkpoints are taken based on the macro average F1 score. The function returns the model weights of the\n",
        "  epoch which has the best score beyond the fifth epoch. The early stopping condition is met when there are\n",
        "  three consecutive falls in the macro average F1 score after the fifth epoch.\n",
        "\n",
        "  :param gridsearch_params: A dictionary containing parameters sourced from a grid search. Key parameters\n",
        "                            include 'dataset' (which determines the chosen method of dataset processing),\n",
        "                            'embeddings', 'batch-size', 'tcn' (which further includes 'layers' and 'dropout-rate'),\n",
        "                            'kernel-size', 'optimiser', and 'learning-rate'.\n",
        "  :returns: A tuple containing (1) the best model weights during training, corresponding to the epoch with\n",
        "            the highest macro average F1 score beyond the fifth epoch, and (2) that highest Macro-weighted\n",
        "            average F1 score.\n",
        "  \"\"\"\n",
        "  if gridsearch_params['dataset'] == 'method 2':\n",
        "    balanced_train_df = augment_train_dataset(train_df.copy(deep=True), aug_train_csv_file)\n",
        "  else:\n",
        "    balanced_train_df = process_training_set(train_df.copy(deep=True), oversample_minority=True, undersample_majority=True, prune=False)\n",
        "\n",
        "  copy_validation_df = validation_df.copy(deep=True)\n",
        "  # apply mean across all embeddings\n",
        "  balanced_train_df['wav2vec_embeddings'] = balanced_train_df['wav2vec_embeddings'].apply(lambda x: torch.mean(x, dim=0))\n",
        "  copy_validation_df['wav2vec_embeddings'] = copy_validation_df['wav2vec_embeddings'].apply(lambda x: torch.mean(x, dim=0))\n",
        "  balanced_train_df['hubert_embeddings'] = balanced_train_df['hubert_embeddings'].apply(lambda x: torch.mean(x, dim=0))\n",
        "  copy_validation_df['hubert_embeddings'] = copy_validation_df['hubert_embeddings'].apply(lambda x: torch.mean(x, dim=0))\n",
        "\n",
        "  wav2vec_train_data, wav2vec_valid_data = balanced_train_df['wav2vec_embeddings'], copy_validation_df['wav2vec_embeddings']\n",
        "  hubert_train_data, hubert_valid_data = balanced_train_df['hubert_embeddings'], copy_validation_df['hubert_embeddings']\n",
        "  train_labels, valid_labels = balanced_train_df['classification'], copy_validation_df['classification']\n",
        "\n",
        "  if gridsearch_params['embeddings'] == 'wav2vec':\n",
        "    train_dataset = EmbeddingsDataset(wav2vec_train_data, train_labels)\n",
        "    valid_dataset = EmbeddingsDataset(wav2vec_valid_data, valid_labels)\n",
        "  else:\n",
        "    train_dataset = EmbeddingsDataset(hubert_train_data, train_labels)\n",
        "    valid_dataset = EmbeddingsDataset(hubert_valid_data, valid_labels)\n",
        "\n",
        "  train_loader = DataLoader(train_dataset, batch_size=gridsearch_params['batch-size'], shuffle=True)\n",
        "  valid_loader = DataLoader(valid_dataset, batch_size=gridsearch_params['batch-size'])\n",
        "  classifier_model = AudioModel(hidden_dims=gridsearch_params['architecture']['layers'], dropout_rate=gridsearch_params['architecture']['dropout-rate'])\n",
        "\n",
        "  if gridsearch_params['optimiser'] == 'Adam':\n",
        "    optimizer = torch.optim.Adam(classifier_model.parameters(), lr=gridsearch_params['learning-rate'])\n",
        "  else:\n",
        "    optimizer = torch.optim.SGD(classifier_model.parameters(), lr=gridsearch_params['learning-rate'], momentum=0.9)\n",
        "\n",
        "  train_losses = []\n",
        "  valid_losses = []\n",
        "  macro_f1_scores = []\n",
        "  best_model_weights = None\n",
        "  best_macro_f1 = -1\n",
        "\n",
        "  MAX_EPOCHS = 20\n",
        "  consecutive_rises = 0  # keep track of consecutive rises in validation loss\n",
        "\n",
        "  for epoch in range(MAX_EPOCHS):\n",
        "      train_loss, train_acc = train(classifier_model, train_loader, optimizer, criterion)\n",
        "      valid_loss, valid_acc, epoch_macro_f1 = evaluate(classifier_model, valid_loader, criterion)\n",
        "\n",
        "      train_losses.append(train_loss)\n",
        "      valid_losses.append(valid_loss)\n",
        "      macro_f1_scores.append(epoch_macro_f1)\n",
        "\n",
        "      if epoch_macro_f1 > best_macro_f1 and epoch >= 5:\n",
        "        best_macro_f1 = epoch_macro_f1\n",
        "        best_model_weights = classifier_model.state_dict()\n",
        "\n",
        "      if epoch > 0 and epoch_macro_f1 < macro_f1_scores[-2]: # early stop if 3 consecutive rises in F1 score\n",
        "          consecutive_rises += 1\n",
        "      else:\n",
        "          consecutive_rises = 0\n",
        "\n",
        "      if consecutive_rises >= 3 and epoch >= 4: # ensure at least 5 epochs are completed\n",
        "          break\n",
        "\n",
        "  validation_performance = max(macro_f1_scores[4:]) # only count losses from the 5th epoch onwards\n",
        "  del balanced_train_df, copy_validation_df\n",
        "  print('Finished training model: ', gridsearch_params, ', with highest Macro-weighted average F1 score: ', validation_performance)\n",
        "\n",
        "  return best_model_weights, validation_performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCl9UUMl8QDW",
        "outputId": "659f0733-7719-480c-c66a-1da8b3e080a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished training model:  {'learning-rate': 0.0005, 'batch-size': 16, 'optimiser': 'Adam', 'architecture': {'layers': [512, 256], 'dropout-rate': 0.2}, 'embeddings': 'wav2vec', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.557764880410171\n",
            "Finished training model:  {'learning-rate': 0.0005, 'batch-size': 16, 'optimiser': 'Adam', 'architecture': {'layers': [512, 256], 'dropout-rate': 0.2}, 'embeddings': 'wav2vec', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.59899389148401\n",
            "Finished training model:  {'learning-rate': 0.0005, 'batch-size': 16, 'optimiser': 'Adam', 'architecture': {'layers': [512, 256], 'dropout-rate': 0.2}, 'embeddings': 'hubert', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.6259073427909227\n",
            "Finished training model:  {'learning-rate': 0.0005, 'batch-size': 16, 'optimiser': 'Adam', 'architecture': {'layers': [512, 256], 'dropout-rate': 0.2}, 'embeddings': 'hubert', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.6832767174173368\n",
            "Finished training model:  {'learning-rate': 0.0005, 'batch-size': 16, 'optimiser': 'Adam', 'architecture': {'layers': [768, 512, 256], 'dropout-rate': 0.3}, 'embeddings': 'wav2vec', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.4518181818181818\n",
            "Finished training model:  {'learning-rate': 0.0005, 'batch-size': 16, 'optimiser': 'Adam', 'architecture': {'layers': [768, 512, 256], 'dropout-rate': 0.3}, 'embeddings': 'wav2vec', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.59467503403087\n",
            "Finished training model:  {'learning-rate': 0.0005, 'batch-size': 16, 'optimiser': 'Adam', 'architecture': {'layers': [768, 512, 256], 'dropout-rate': 0.3}, 'embeddings': 'hubert', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.6375\n",
            "Finished training model:  {'learning-rate': 0.0005, 'batch-size': 16, 'optimiser': 'Adam', 'architecture': {'layers': [768, 512, 256], 'dropout-rate': 0.3}, 'embeddings': 'hubert', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.6884240894338263\n",
            "Finished training model:  {'learning-rate': 0.0005, 'batch-size': 16, 'optimiser': 'Adam', 'architecture': {'layers': [1024, 768, 512, 256], 'dropout-rate': 0.4}, 'embeddings': 'wav2vec', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.47660154475238536\n",
            "Finished training model:  {'learning-rate': 0.0005, 'batch-size': 16, 'optimiser': 'Adam', 'architecture': {'layers': [1024, 768, 512, 256], 'dropout-rate': 0.4}, 'embeddings': 'wav2vec', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.575\n",
            "Finished training model:  {'learning-rate': 0.0005, 'batch-size': 16, 'optimiser': 'Adam', 'architecture': {'layers': [1024, 768, 512, 256], 'dropout-rate': 0.4}, 'embeddings': 'hubert', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.6259073427909227\n",
            "Finished training model:  {'learning-rate': 0.0005, 'batch-size': 16, 'optimiser': 'Adam', 'architecture': {'layers': [1024, 768, 512, 256], 'dropout-rate': 0.4}, 'embeddings': 'hubert', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.5708384419503915\n",
            "Finished training model:  {'learning-rate': 0.0005, 'batch-size': 16, 'optimiser': 'SGD', 'architecture': {'layers': [512, 256], 'dropout-rate': 0.2}, 'embeddings': 'wav2vec', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.08860759493670886\n",
            "Finished training model:  {'learning-rate': 0.0005, 'batch-size': 16, 'optimiser': 'SGD', 'architecture': {'layers': [512, 256], 'dropout-rate': 0.2}, 'embeddings': 'wav2vec', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.5505267264923916\n",
            "Finished training model:  {'learning-rate': 0.0005, 'batch-size': 16, 'optimiser': 'SGD', 'architecture': {'layers': [512, 256], 'dropout-rate': 0.2}, 'embeddings': 'hubert', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.08860759493670886\n",
            "Finished training model:  {'learning-rate': 0.0005, 'batch-size': 16, 'optimiser': 'SGD', 'architecture': {'layers': [512, 256], 'dropout-rate': 0.2}, 'embeddings': 'hubert', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.6578217821782177\n",
            "Finished training model:  {'learning-rate': 0.0005, 'batch-size': 16, 'optimiser': 'SGD', 'architecture': {'layers': [768, 512, 256], 'dropout-rate': 0.3}, 'embeddings': 'wav2vec', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.08860759493670886\n",
            "Finished training model:  {'learning-rate': 0.0005, 'batch-size': 16, 'optimiser': 'SGD', 'architecture': {'layers': [768, 512, 256], 'dropout-rate': 0.3}, 'embeddings': 'wav2vec', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.5913154533844189\n",
            "Finished training model:  {'learning-rate': 0.0005, 'batch-size': 16, 'optimiser': 'SGD', 'architecture': {'layers': [768, 512, 256], 'dropout-rate': 0.3}, 'embeddings': 'hubert', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.08860759493670886\n",
            "Finished training model:  {'learning-rate': 0.0005, 'batch-size': 16, 'optimiser': 'SGD', 'architecture': {'layers': [768, 512, 256], 'dropout-rate': 0.3}, 'embeddings': 'hubert', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.7032706361567029\n",
            "Finished training model:  {'learning-rate': 0.0005, 'batch-size': 16, 'optimiser': 'SGD', 'architecture': {'layers': [1024, 768, 512, 256], 'dropout-rate': 0.4}, 'embeddings': 'wav2vec', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.08860759493670886\n",
            "Finished training model:  {'learning-rate': 0.0005, 'batch-size': 16, 'optimiser': 'SGD', 'architecture': {'layers': [1024, 768, 512, 256], 'dropout-rate': 0.4}, 'embeddings': 'wav2vec', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.4744525547445256\n",
            "Finished training model:  {'learning-rate': 0.0005, 'batch-size': 16, 'optimiser': 'SGD', 'architecture': {'layers': [1024, 768, 512, 256], 'dropout-rate': 0.4}, 'embeddings': 'hubert', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.08860759493670886\n",
            "Finished training model:  {'learning-rate': 0.0005, 'batch-size': 16, 'optimiser': 'SGD', 'architecture': {'layers': [1024, 768, 512, 256], 'dropout-rate': 0.4}, 'embeddings': 'hubert', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.6845885439217584\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'Adam', 'architecture': {'layers': [512, 256], 'dropout-rate': 0.2}, 'embeddings': 'wav2vec', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.4926536731634183\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'Adam', 'architecture': {'layers': [512, 256], 'dropout-rate': 0.2}, 'embeddings': 'wav2vec', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.634920634920635\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'Adam', 'architecture': {'layers': [512, 256], 'dropout-rate': 0.2}, 'embeddings': 'hubert', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.6225806451612903\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'Adam', 'architecture': {'layers': [512, 256], 'dropout-rate': 0.2}, 'embeddings': 'hubert', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.6345430634704565\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'Adam', 'architecture': {'layers': [768, 512, 256], 'dropout-rate': 0.3}, 'embeddings': 'wav2vec', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.5017404898384575\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'Adam', 'architecture': {'layers': [768, 512, 256], 'dropout-rate': 0.3}, 'embeddings': 'wav2vec', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.6049382716049383\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'Adam', 'architecture': {'layers': [768, 512, 256], 'dropout-rate': 0.3}, 'embeddings': 'hubert', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.6480651731160896\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'Adam', 'architecture': {'layers': [768, 512, 256], 'dropout-rate': 0.3}, 'embeddings': 'hubert', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.6552479433563405\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'Adam', 'architecture': {'layers': [1024, 768, 512, 256], 'dropout-rate': 0.4}, 'embeddings': 'wav2vec', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.49543676662320724\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'Adam', 'architecture': {'layers': [1024, 768, 512, 256], 'dropout-rate': 0.4}, 'embeddings': 'wav2vec', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.606904857486953\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'Adam', 'architecture': {'layers': [1024, 768, 512, 256], 'dropout-rate': 0.4}, 'embeddings': 'hubert', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.6064147259796402\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'Adam', 'architecture': {'layers': [1024, 768, 512, 256], 'dropout-rate': 0.4}, 'embeddings': 'hubert', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.6331635016211209\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'SGD', 'architecture': {'layers': [512, 256], 'dropout-rate': 0.2}, 'embeddings': 'wav2vec', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.40049487451396254\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'SGD', 'architecture': {'layers': [512, 256], 'dropout-rate': 0.2}, 'embeddings': 'wav2vec', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.5761386876660487\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'SGD', 'architecture': {'layers': [512, 256], 'dropout-rate': 0.2}, 'embeddings': 'hubert', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.505564764385508\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'SGD', 'architecture': {'layers': [512, 256], 'dropout-rate': 0.2}, 'embeddings': 'hubert', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.6680044402698317\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'SGD', 'architecture': {'layers': [768, 512, 256], 'dropout-rate': 0.3}, 'embeddings': 'wav2vec', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.08860759493670886\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'SGD', 'architecture': {'layers': [768, 512, 256], 'dropout-rate': 0.3}, 'embeddings': 'wav2vec', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.5879253184435527\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'SGD', 'architecture': {'layers': [768, 512, 256], 'dropout-rate': 0.3}, 'embeddings': 'hubert', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.08860759493670886\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'SGD', 'architecture': {'layers': [768, 512, 256], 'dropout-rate': 0.3}, 'embeddings': 'hubert', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.62615518744551\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'SGD', 'architecture': {'layers': [1024, 768, 512, 256], 'dropout-rate': 0.4}, 'embeddings': 'wav2vec', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.08860759493670886\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'SGD', 'architecture': {'layers': [1024, 768, 512, 256], 'dropout-rate': 0.4}, 'embeddings': 'wav2vec', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.5772166764533176\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'SGD', 'architecture': {'layers': [1024, 768, 512, 256], 'dropout-rate': 0.4}, 'embeddings': 'hubert', 'dataset': 'method 1'} , with highest Macro-weighted average F1 score:  0.08860759493670886\n",
            "Finished training model:  {'learning-rate': 0.001, 'batch-size': 16, 'optimiser': 'SGD', 'architecture': {'layers': [1024, 768, 512, 256], 'dropout-rate': 0.4}, 'embeddings': 'hubert', 'dataset': 'method 2'} , with highest Macro-weighted average F1 score:  0.6933046335108191\n",
            "\n",
            "Optimal hyperparameters for grid search with macro average F1 of  0.7032706361567029  :\n",
            "{'learning-rate': 0.0005, 'batch-size': 16, 'optimiser': 'SGD', 'architecture': {'layers': [768, 512, 256], 'dropout-rate': 0.3}, 'embeddings': 'hubert', 'dataset': 'method 2'}\n"
          ]
        }
      ],
      "source": [
        "from itertools import product\n",
        "\n",
        "def grid_search(param_grid):\n",
        "    \"\"\"\n",
        "    Conducts a grid search over the specified parameter space to find the best parameters that maximize\n",
        "    the performance of the `train_model` function.\n",
        "\n",
        "    For every combination of parameters in the grid, the function trains the model and saves the best\n",
        "    model weights (based on macro average F1 score beyond the fifth epoch) if the current combination\n",
        "    yields better performance than previous ones. This approach ensures that the optimal hyperparameters\n",
        "    and corresponding model weights are identified and saved.\n",
        "\n",
        "    :param param_grid: A dictionary where keys are parameter names and values are lists of possible values\n",
        "                       for that parameter.\n",
        "    :returns: A tuple containing (1) a dictionary of the best parameters identified during the grid search\n",
        "              and (2) the highest macro average F1 score obtained using those best parameters.\n",
        "    \"\"\"\n",
        "    # create a list of all parameter combinations\n",
        "    all_params = [dict(zip(param_grid.keys(), v)) for v in product(*param_grid.values())]\n",
        "    best_params = None\n",
        "    best_performance = float('-inf')\n",
        "\n",
        "    for params in all_params:\n",
        "        best_weights, performance = train_model(params)\n",
        "        if performance > best_performance:\n",
        "            best_performance = performance\n",
        "            best_params = params\n",
        "            torch.save(best_weights, SAVE_WEIGHTS_PATH)\n",
        "\n",
        "    return best_params, best_performance\n",
        "\n",
        "best_hyperparameters, best_performance = grid_search(param_grid)\n",
        "print('\\nOptimal hyperparameters for grid search with macro average F1 of ',  best_performance,' :')\n",
        "print(best_hyperparameters)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
