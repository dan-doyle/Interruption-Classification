{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aPWXEi-FTGl"
      },
      "outputs": [],
      "source": [
        "# when executed in a Google Colab setting, we must install the required libraries\n",
        "\n",
        "# !pip install torch\n",
        "# !pip install os\n",
        "# !pip install transformers\n",
        "# !pip install numpy\n",
        "# !pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCoFa99OFjv1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_sequence\n",
        "from torchaudio.transforms import MelSpectrogram, MFCC\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import pickle\n",
        "import base64\n",
        "import matplotlib.pyplot as plt\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmiSqAaYHX47",
        "outputId": "b0d58b74-f801-41da-938a-b80aa723887c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fc185ef4f90>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "DATASET_FILEPATH = './drive/MyDrive/Thesis/'\n",
        "DATASET_SEED = 2\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "SAVE_WEIGHTS_PATH = os.path.join(DATASET_FILEPATH, 'weights-and-graphs/baseline-vad/model.pth')\n",
        "SAVE_PLOTS_PATH = os.path.join(DATASET_FILEPATH, 'weights-and-graphs/baseline-vad/loss.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0BZaE88hV-r"
      },
      "outputs": [],
      "source": [
        "train_csv_file = os.path.join(DATASET_FILEPATH, f'base/{DATASET_SEED}/processed/train_dataset.csv')\n",
        "validation_csv_file = os.path.join(DATASET_FILEPATH, f'base/{DATASET_SEED}/processed/validation_dataset.csv')\n",
        "aug_train_csv_file = os.path.join(DATASET_FILEPATH, f'{BASE}/aug-dataset/processed/train_dataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gv9Y8tRFba9",
        "outputId": "f9ab803c-b38f-44c6-f567-df3bd33c24fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print('Device: ', device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlrdyKpSFtim"
      },
      "outputs": [],
      "source": [
        "selected_columns = ['audio_file_name', 'classification']\n",
        "\n",
        "train_df = pd.read_csv(train_csv_file, usecols=selected_columns)\n",
        "validation_df = pd.read_csv(validation_csv_file, usecols=selected_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbsKgHA9F6r3"
      },
      "outputs": [],
      "source": [
        "def process_training_set(train_df, oversample_minority=False, undersample_majority=False):\n",
        "  \"\"\"\n",
        "  Re-sample the training dataset, with options to oversample minority class and undersample majority class based on audio lengths.\n",
        "\n",
        "  :param train_df: DataFrame containing the training data with columns ['classification', 'audio_file_name'] among others.\n",
        "  :param oversample_minority: Boolean, if True, the minority class (classification == 0) is duplicated to balance the dataset.\n",
        "  :param undersample_majority: Boolean, if True, majority class data with audio lengths above a threshold (specified by DROP_SEGMENTS) are dropped.\n",
        "  :returns: DataFrame with the desired processed training data.\n",
        "  \"\"\"\n",
        "  if oversample_minority:\n",
        "    class_0 = train_df[train_df['classification'] == 0]\n",
        "    train_df = pd.concat([train_df, class_0])\n",
        "  if undersample_majority:\n",
        "    DROP_SEGMENTS = 5\n",
        "    def get_audio_length_group(file_name):\n",
        "        return int(re.findall(r'\\d+', file_name)[-1])\n",
        "    train_df['audio_length_group'] = train_df['audio_file_name'].apply(get_audio_length_group)\n",
        "    train_df = train_df[train_df['audio_length_group'] <= DROP_SEGMENTS]\n",
        "    train_df = train_df.drop(columns=['audio_length_group'])\n",
        "\n",
        "  # some indices are duplicated / removed so we have to reset them\n",
        "  train_df.reset_index(drop=True, inplace=True)\n",
        "  return train_df\n",
        "\n",
        "def print_dataset_balance(df):\n",
        "    \"\"\"\n",
        "    Prints the balance of classifications in a given dataset.\n",
        "\n",
        "    :param df: DataFrame containing the data with a 'classification' column.\n",
        "    \"\"\"\n",
        "    classification_counts = df['classification'].value_counts().reset_index()\n",
        "    classification_counts.columns = ['classification', 'count']\n",
        "    total_rows = classification_counts['count'].sum()\n",
        "    classification_counts['percentage'] = (classification_counts['count'] / total_rows) * 100\n",
        "    classification_counts['percentage'] = classification_counts['percentage'].round(1)\n",
        "    print(classification_counts)\n",
        "\n",
        "def augment_train_dataset(df, augmented_df_filepath):\n",
        "  \"\"\"\n",
        "  Introduce additional 'non-interruption' samples to the dataset, which have been extracted from the GAP dataset with an LLM.\n",
        "\n",
        "  :param df: Original DataFrame containing the training data.\n",
        "  :param augmented_df_filepath: Filepath to the CSV containing the augmented data.\n",
        "  :returns: A combined DataFrame of the original and augmented training data.\n",
        "  \"\"\"\n",
        "  selected_columns = ['audio_file_name','classification', 'wav2vec_embeddings', 'hubert_embeddings']\n",
        "  aug_train_df = pd.read_csv(aug_train_csv_file, usecols=selected_columns, converters={'wav2vec_embeddings': to_tensor, 'hubert_embeddings' : to_tensor})\n",
        "  augmented_df = pd.concat([df, aug_train_df], ignore_index=True)\n",
        "  return augmented_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98LHryzyGLXR"
      },
      "outputs": [],
      "source": [
        "AUGMENT = True\n",
        "\n",
        "if AUGMENT:\n",
        "  print(\"Length of the DataFrame before:\", len(train_df))\n",
        "  train_df = augment_train_dataset(train_df, aug_train_csv_file)\n",
        "  print(\"Length of the DataFrame after:\", len(train_df))\n",
        "else:\n",
        "  train_df = process_training_set(train_df, oversample_minority=True, undersample_majority=True, prune=False)\n",
        "print_dataset_balance(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ViRRjTHgHaNn"
      },
      "outputs": [],
      "source": [
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, audio_file_name, labels):\n",
        "        self.labels = labels\n",
        "\n",
        "        # Precompute and store all MFCC features\n",
        "        self.audio_features = [self.extract_mfcc(os.path.join('./drive/MyDrive/Thesis/audio', fname)) for fname in audio_file_name]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.audio_features[idx], self.labels[idx]\n",
        "\n",
        "    def extract_mfcc(self, audio_path):\n",
        "        waveform, sample_rate = torchaudio.load(audio_path)\n",
        "        mfcc_transform = MFCC(\n",
        "            sample_rate=sample_rate,\n",
        "            n_mfcc=13,\n",
        "            melkwargs={\"n_fft\": 400, \"hop_length\": 160, \"n_mels\": 23, \"center\": False},\n",
        "        )\n",
        "        mfcc = mfcc_transform(waveform).squeeze().transpose(0, 1)\n",
        "        return mfcc\n",
        "\n",
        "audio_train_data, audio_valid_data = train_df['audio_file_name'], validation_df['audio_file_name']\n",
        "train_labels, valid_labels = train_df['classification'], validation_df['classification']\n",
        "\n",
        "train_dataset = AudioDataset(audio_train_data, train_labels)\n",
        "valid_dataset = AudioDataset(audio_valid_data, valid_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_e3DUwfZHs6g"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Function to be passed to the DataLoader class which processes a batch of data points before being passed to the model in training. The LSTM must have all batch samples of equal length.\n",
        "\n",
        "    :param batch: array of data points in the dataset.\n",
        "    \"\"\"\n",
        "    features, labels = zip(*batch)\n",
        "    labels = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "    # Convert stereo to mono by averaging across the channel dimension\n",
        "    features = [feature.mean(1) for feature in features]\n",
        "    lengths = [feature.shape[1] for feature in features]  # Updated index for time dimension\n",
        "\n",
        "    # Transpose such that time dimension is first\n",
        "    features = [feature.transpose(0, 1) for feature in features]\n",
        "    features = pad_sequence(features, batch_first=True)\n",
        "\n",
        "    return features, labels, lengths\n",
        "\n",
        "# Change below for data augmentation\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNOijgOpvJn-"
      },
      "outputs": [],
      "source": [
        "# LSTM Classifier\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout_rate):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.LSTM(input_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout_rate if n_layers > 1 else 0)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, embedding, lengths):\n",
        "        packed = pack_padded_sequence(embedding, lengths, batch_first=True, enforce_sorted=False)\n",
        "        packed_output, (hidden, cell) = self.rnn(packed)\n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
        "        return self.fc(hidden)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AEPuVPmH6mz"
      },
      "outputs": [],
      "source": [
        "INPUT_DIMENSION = 13\n",
        "NUM_HIDDEN_UNITS = 64\n",
        "OUTPUT_DIMENSION = 1\n",
        "NUM_LSTM_LAYERS = 1\n",
        "BI_DIRECTIONAL = True\n",
        "DROPOUT_RATE = 0.1\n",
        "\n",
        "model = Classifier(INPUT_DIMENSION, NUM_HIDDEN_UNITS, OUTPUT_DIMENSION, NUM_LSTM_LAYERS, BI_DIRECTIONAL, DROPOUT_RATE).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qg4NQLTvH85U"
      },
      "outputs": [],
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Receives predicted values and true labels and computes the average accuracy of the predictions.\n",
        "\n",
        "    :param preds: Tensor of predicted values.\n",
        "    :param y: Tensor of true labels.\n",
        "    :returns: Accuracy as a floating point value.\n",
        "    \"\"\"\n",
        "\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float()  #convert into float for division\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \"\"\"\n",
        "    Evaluate the model's performance on a given dataset. This is used for the validation\n",
        "\n",
        "    :param model: PyTorch model to be evaluated.\n",
        "    :param iterator: Iterator that provides batches of data for evaluation.\n",
        "    :param criterion: Loss function used to compute the loss during evaluation.\n",
        "    :returns: Tuple containing average loss and average accuracy over all batches.\n",
        "    \"\"\"\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for audio_features, labels, lengths in iterator:\n",
        "            audio_features, labels = audio_features.to(device), labels.to(device)\n",
        "            predictions = model(audio_features, lengths).squeeze(1)\n",
        "            loss = criterion(predictions, labels.float())\n",
        "            acc = binary_accuracy(predictions, labels)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    \"\"\"\n",
        "    Train the model for one epoch on the dataset.\n",
        "\n",
        "    :param model: The PyTorch model to be trained.\n",
        "    :param iterator: Iterator that provides batches of data for training.\n",
        "    :param optimizer: Optimizer used to update the model's parameters.\n",
        "    :param criterion: Loss function used to compute the loss during training.\n",
        "    :returns: Tuple containing average loss, average accuracy over all batches.\n",
        "    \"\"\"\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.train()\n",
        "\n",
        "    for audio_features, labels, lengths in iterator:\n",
        "        audio_features, labels = audio_features.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(audio_features, lengths).squeeze(1)\n",
        "        loss = criterion(predictions, labels.float())\n",
        "        acc = binary_accuracy(predictions, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFt9bm7nIAVn"
      },
      "outputs": [],
      "source": [
        "N_EPOCHS = 20\n",
        "\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "for epoch in range(N_EPOCHS):\n",
        "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_loader, criterion)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
        "\n",
        "torch.save(model.state_dict(), SAVE_WEIGHTS_PATH)\n",
        "print('Model weights saved')\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, N_EPOCHS+1), train_losses, color='blue', label='Training Loss')\n",
        "plt.plot(range(1, N_EPOCHS+1), valid_losses, color='red', label='Validation Loss')\n",
        "\n",
        "plt.xticks(range(1, N_EPOCHS+1))\n",
        "plt.yticks([i/20 for i in range(int(max(train_losses+valid_losses)*20)+1)])\n",
        "\n",
        "plt.title('Average')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "# Save the plot to the './resources' directory\n",
        "plt.savefig(SAVE_PLOTS_PATH)\n",
        "print('Plot of loss saved')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
